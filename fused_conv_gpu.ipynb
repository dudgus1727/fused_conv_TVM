{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 227, 227, 3]\n",
            "[64, 3, 3, 3]\n",
            "[64, 3, 3, 64]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tvm\n",
        "from tvm import te\n",
        "\n",
        "# The sizes of inputs and filters\n",
        "batch =1 \n",
        "img_size = 227\n",
        "img_ch = 3\n",
        "filter_size = 3\n",
        "filter_num = 64\n",
        "\n",
        "# Algorithm\n",
        "In = te.placeholder((batch, img_size, img_size, img_ch), name=\"In\")\n",
        "W1 = te.placeholder((filter_num,filter_size, filter_size, img_ch), name=\"W1\")\n",
        "W2 = te.placeholder((filter_num, filter_size, filter_size, filter_num), name=\"W2\")\n",
        "\n",
        "print(In.shape)\n",
        "print(W1.shape)\n",
        "print(W2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Conv(Input, Kernel, stride, pad, name):\n",
        "    batch_size, in_size, _, in_channel = Input.shape\n",
        "    out_channel, kernel_size, _, _ = Kernel.shape\n",
        "\n",
        "    out_size = (in_size - kernel_size + 2 * pad) // stride + 1\n",
        "    # Pad input\n",
        "\n",
        "    # if pad > 0:\n",
        "    #     Input = te.compute(\n",
        "    #         (in_size + 2 * pad, in_size + 2 * pad, in_channel, batch_size),\n",
        "    #         lambda yy, xx, cc, nn: tvm.tir.if_then_else(\n",
        "    #             tvm.tir.all(yy >= pad, yy - pad < in_size, xx >= pad, xx - pad < in_size),\n",
        "    #             Input[yy - pad, xx - pad, cc, nn],\n",
        "    #             tvm.tir.const(0.0, \"float32\"),),\n",
        "    #         name=name + \"_pad\",)\n",
        "    # Create reduction variables\n",
        "    rc = te.reduce_axis((0, in_channel), name=name+\"_rc\")\n",
        "    ry = te.reduce_axis((0, kernel_size), name=name+\"_ry\")\n",
        "    rx = te.reduce_axis((0, kernel_size), name=name+\"_rx\")\n",
        "    # Compute the convolution\n",
        "    Conv = te.compute(\n",
        "        (batch_size, out_size, out_size, out_channel),\n",
        "        lambda nn, yy, xx, ff: te.sum(\n",
        "            Input[nn, yy * stride + ry, xx * stride + rx, rc] * Kernel[ff, ry, rx, rc], axis=[ry, rx, rc]\n",
        "        ),\n",
        "        name=name+'_conv',\n",
        "    )\n",
        "    return Conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv1 = Conv(In, W1, stride=2, pad=0, name='l1')\n",
        "conv2 = Conv(conv1, W2, stride=2, pad=0, name='l2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Origin Schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@main = primfn(In_1: handle, W1_1: handle, W2_1: handle, l2_conv_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {In: Buffer(In_2: Pointer(float32), float32, [1, 227, 227, 3], []),\n",
            "             W1: Buffer(W1_2: Pointer(float32), float32, [64, 3, 3, 3], []),\n",
            "             W2: Buffer(W2_2: Pointer(float32), float32, [64, 3, 3, 64], []),\n",
            "             l2_conv: Buffer(l2_conv_2: Pointer(float32), float32, [1, 56, 56, 64], [])}\n",
            "  buffer_map = {In_1: In, W1_1: W1, W2_1: W2, l2_conv_1: l2_conv} {\n",
            "  allocate(l1_conv: Pointer(global float32), float32, [817216]), storage_scope = global {\n",
            "    for (yy: int32, 0, 113) {\n",
            "      for (xx: int32, 0, 113) {\n",
            "        for (ff: int32, 0, 64) {\n",
            "          l1_conv_1: Buffer(l1_conv, float32, [817216], [])[(((yy*7232) + (xx*64)) + ff)] = 0f32\n",
            "          for (l1_ry: int32, 0, 3) {\n",
            "            for (l1_rx: int32, 0, 3) {\n",
            "              for (l1_rc: int32, 0, 3) {\n",
            "                let cse_var_2: int32 = (l1_rx*3)\n",
            "                let cse_var_1: int32 = (((yy*7232) + (xx*64)) + ff)\n",
            "                l1_conv_1[cse_var_1] = (l1_conv_1[cse_var_1] + (In_3: Buffer(In_2, float32, [154587], [])[(((((yy*1362) + (l1_ry*681)) + (xx*6)) + cse_var_2) + l1_rc)]*W1_3: Buffer(W1_2, float32, [1728], [])[((((ff*27) + (l1_ry*9)) + cse_var_2) + l1_rc)]))\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (yy_1: int32, 0, 56) {\n",
            "      for (xx_1: int32, 0, 56) {\n",
            "        for (ff_1: int32, 0, 64) {\n",
            "          l2_conv_3: Buffer(l2_conv_2, float32, [200704], [])[(((yy_1*3584) + (xx_1*64)) + ff_1)] = 0f32\n",
            "          for (l2_ry: int32, 0, 3) {\n",
            "            for (l2_rx: int32, 0, 3) {\n",
            "              for (l2_rc: int32, 0, 64) {\n",
            "                let cse_var_4: int32 = (l2_rx*64)\n",
            "                let cse_var_3: int32 = (((yy_1*3584) + (xx_1*64)) + ff_1)\n",
            "                l2_conv_3[cse_var_3] = (l2_conv_3[cse_var_3] + (l1_conv_1[(((((yy_1*14464) + (l2_ry*7232)) + (xx_1*128)) + cse_var_4) + l2_rc)]*W2_3: Buffer(W2_2, float32, [36864], [])[((((ff_1*576) + (l2_ry*192)) + cse_var_4) + l2_rc)]))\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "s = te.create_schedule(conv2.op)\n",
        "print(tvm.lower(s, [In, W1, W2, conv2], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "# s = te.create_schedule(conv2.op)\n",
        "# xo, yo, xi, yi = s[conv2].tile(conv2.op.axis[0], conv2.op.axis[1], x_factor=28, y_factor=28)\n",
        "# print(tvm.lower(s, [In, W1, W2, conv2], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@main = primfn(In_1: handle, W1_1: handle, W2_1: handle, l2_conv_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {In: Buffer(In_2: Pointer(float32), float32, [1, 227, 227, 3], []),\n",
            "             W1: Buffer(W1_2: Pointer(float32), float32, [64, 3, 3, 3], []),\n",
            "             W2: Buffer(W2_2: Pointer(float32), float32, [64, 3, 3, 64], []),\n",
            "             l2_conv: Buffer(l2_conv_2: Pointer(float32), float32, [1, 56, 56, 64], [])}\n",
            "  buffer_map = {In_1: In, W1_1: W1, W2_1: W2, l2_conv_1: l2_conv} {\n",
            "  allocate(In.shared: Pointer(shared float32), float32, [817216]), storage_scope = shared;\n",
            "  allocate(In.shared.local: Pointer(local float32), float32, [154587]), storage_scope = local;\n",
            "  allocate(W1.shared: Pointer(shared float32), float32, [1728]), storage_scope = shared;\n",
            "  allocate(W1.shared.local: Pointer(local float32), float32, [1728]), storage_scope = local;\n",
            "  allocate(l1_conv: Pointer(global float32), float32, [817216]), storage_scope = global;\n",
            "  allocate(l1_conv.d.shared.local: Pointer(local float32), float32, [817216]), storage_scope = local;\n",
            "  allocate(W2.shared: Pointer(shared float32), float32, [36864]), storage_scope = shared;\n",
            "  allocate(W2.shared.local: Pointer(local float32), float32, [36864]), storage_scope = local;\n",
            "  allocate(l2_conv.local: Pointer(local float32), float32, [200704]), storage_scope = local {\n",
            "    for (ax1: int32, 0, 227) {\n",
            "      for (ax2: int32, 0, 227) {\n",
            "        for (ax3: int32, 0, 3) {\n",
            "          let cse_var_1: int32 = (((ax1*681) + (ax2*3)) + ax3)\n",
            "          In.shared_1: Buffer(In.shared, float32, [154587], [], scope=\"shared\")[cse_var_1] = In_3: Buffer(In_2, float32, [154587], [])[cse_var_1]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax1_1: int32, 0, 227) {\n",
            "      for (ax2_1: int32, 0, 227) {\n",
            "        for (ax3_1: int32, 0, 3) {\n",
            "          let cse_var_2: int32 = (((ax1_1*681) + (ax2_1*3)) + ax3_1)\n",
            "          In.shared.local_1: Buffer(In.shared.local, float32, [154587], [], scope=\"local\")[cse_var_2] = In.shared_1[cse_var_2]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0: int32, 0, 64) {\n",
            "      for (ax1_2: int32, 0, 3) {\n",
            "        for (ax2_2: int32, 0, 3) {\n",
            "          for (ax3_2: int32, 0, 3) {\n",
            "            let cse_var_3: int32 = ((((ax0*27) + (ax1_2*9)) + (ax2_2*3)) + ax3_2)\n",
            "            W1.shared_1: Buffer(W1.shared, float32, [1728], [], scope=\"shared\")[cse_var_3] = W1_3: Buffer(W1_2, float32, [1728], [])[cse_var_3]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_1: int32, 0, 64) {\n",
            "      for (ax1_3: int32, 0, 3) {\n",
            "        for (ax2_3: int32, 0, 3) {\n",
            "          for (ax3_3: int32, 0, 3) {\n",
            "            let cse_var_4: int32 = ((((ax0_1*27) + (ax1_3*9)) + (ax2_3*3)) + ax3_3)\n",
            "            W1.shared.local_1: Buffer(W1.shared.local, float32, [1728], [], scope=\"local\")[cse_var_4] = W1.shared_1[cse_var_4]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (yy.c: int32, 0, 113) {\n",
            "      for (xx.c: int32, 0, 113) {\n",
            "        for (ff.c: int32, 0, 64) {\n",
            "          In.shared_2: Buffer(In.shared, float32, [817216], [], scope=\"shared\")[(((yy.c*7232) + (xx.c*64)) + ff.c)] = 0f32\n",
            "          for (l1_ry: int32, 0, 3) {\n",
            "            for (l1_rx: int32, 0, 3) {\n",
            "              for (l1_rc: int32, 0, 3) {\n",
            "                let cse_var_6: int32 = (l1_rx*3)\n",
            "                let cse_var_5: int32 = (((yy.c*7232) + (xx.c*64)) + ff.c)\n",
            "                In.shared_2[cse_var_5] = (In.shared_2[cse_var_5] + (In.shared.local_1[(((((yy.c*1362) + (l1_ry*681)) + (xx.c*6)) + cse_var_6) + l1_rc)]*W1.shared.local_1[((((ff.c*27) + (l1_ry*9)) + cse_var_6) + l1_rc)]))\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (yy: int32, 0, 113) {\n",
            "      for (xx: int32, 0, 113) {\n",
            "        for (ff: int32, 0, 64) {\n",
            "          let cse_var_7: int32 = (((yy*7232) + (xx*64)) + ff)\n",
            "          l1_conv_1: Buffer(l1_conv, float32, [817216], [])[cse_var_7] = In.shared_2[cse_var_7]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax1_4: int32, 0, 113) {\n",
            "      for (ax2_4: int32, 0, 113) {\n",
            "        for (ax3_4: int32, 0, 64) {\n",
            "          let cse_var_8: int32 = (((ax1_4*7232) + (ax2_4*64)) + ax3_4)\n",
            "          In.shared_3: Buffer(In.shared, float32, [817216], [], scope=\"shared\")[cse_var_8] = l1_conv_1[cse_var_8]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax1_5: int32, 0, 113) {\n",
            "      for (ax2_5: int32, 0, 113) {\n",
            "        for (ax3_5: int32, 0, 64) {\n",
            "          let cse_var_9: int32 = (((ax1_5*7232) + (ax2_5*64)) + ax3_5)\n",
            "          l1_conv.d.shared.local_1: Buffer(l1_conv.d.shared.local, float32, [817216], [], scope=\"local\")[cse_var_9] = In.shared_3[cse_var_9]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_2: int32, 0, 64) {\n",
            "      for (ax1_6: int32, 0, 3) {\n",
            "        for (ax2_6: int32, 0, 3) {\n",
            "          for (ax3_6: int32, 0, 64) {\n",
            "            let cse_var_10: int32 = ((((ax0_2*576) + (ax1_6*192)) + (ax2_6*64)) + ax3_6)\n",
            "            W2.shared_1: Buffer(W2.shared, float32, [36864], [], scope=\"shared\")[cse_var_10] = W2_3: Buffer(W2_2, float32, [36864], [])[cse_var_10]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_3: int32, 0, 64) {\n",
            "      for (ax1_7: int32, 0, 3) {\n",
            "        for (ax2_7: int32, 0, 3) {\n",
            "          for (ax3_7: int32, 0, 64) {\n",
            "            let cse_var_11: int32 = ((((ax0_3*576) + (ax1_7*192)) + (ax2_7*64)) + ax3_7)\n",
            "            W2.shared.local_1: Buffer(W2.shared.local, float32, [36864], [], scope=\"local\")[cse_var_11] = W2.shared_1[cse_var_11]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (yy.c_1: int32, 0, 56) {\n",
            "      for (xx.c_1: int32, 0, 56) {\n",
            "        for (ff.c_1: int32, 0, 64) {\n",
            "          l2_conv.local_1: Buffer(l2_conv.local, float32, [200704], [], scope=\"local\")[(((yy.c_1*3584) + (xx.c_1*64)) + ff.c_1)] = 0f32\n",
            "          for (l2_ry: int32, 0, 3) {\n",
            "            for (l2_rx: int32, 0, 3) {\n",
            "              for (l2_rc: int32, 0, 64) {\n",
            "                let cse_var_13: int32 = (l2_rx*64)\n",
            "                let cse_var_12: int32 = (((yy.c_1*3584) + (xx.c_1*64)) + ff.c_1)\n",
            "                l2_conv.local_1[cse_var_12] = (l2_conv.local_1[cse_var_12] + (l1_conv.d.shared.local_1[(((((yy.c_1*14464) + (l2_ry*7232)) + (xx.c_1*128)) + cse_var_13) + l2_rc)]*W2.shared.local_1[((((ff.c_1*576) + (l2_ry*192)) + cse_var_13) + l2_rc)]))\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (yy_1: int32, 0, 56) {\n",
            "      for (xx_1: int32, 0, 56) {\n",
            "        for (ff_1: int32, 0, 64) {\n",
            "          let cse_var_14: int32 = (((yy_1*3584) + (xx_1*64)) + ff_1)\n",
            "          l2_conv_3: Buffer(l2_conv_2, float32, [200704], [])[cse_var_14] = l2_conv.local_1[cse_var_14]\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "s = te.create_schedule(conv2.op)\n",
        "# s[conv1].compute_at(s[conv2], conv2.op.axis[2])\n",
        "# s[conv1].compute_inline()  # compute Apad inline\n",
        "\n",
        "SI = s.cache_read(In, \"shared\", [conv1])\n",
        "SW1 = s.cache_read(W1, \"shared\", [conv1])\n",
        "LI = s.cache_read(SI, \"local\", [conv1])\n",
        "LW1 = s.cache_read(SW1, \"local\", [conv1])\n",
        "LC1 = s.cache_write(conv1, \"shared\")\n",
        "\n",
        "SA = s.cache_read(conv1, \"shared\", [conv2])\n",
        "SW2 = s.cache_read(W2, \"shared\", [conv2])\n",
        "LA = s.cache_read(SA, \"local\", [conv2])\n",
        "LW2 = s.cache_read(SW2, \"local\", [conv2])\n",
        "LC2 = s.cache_write(conv2, \"local\")\n",
        "\n",
        "\n",
        "print(tvm.lower(s, [In, W1, W2, conv2], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@main = primfn(In_1: handle, W1_1: handle, W2_1: handle, l2_conv_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {In: Buffer(In_2: Pointer(float32), float32, [1, 227, 227, 3], []),\n",
            "             W1: Buffer(W1_2: Pointer(float32), float32, [64, 3, 3, 3], []),\n",
            "             W2: Buffer(W2_2: Pointer(float32), float32, [64, 3, 3, 64], []),\n",
            "             l2_conv: Buffer(l2_conv_2: Pointer(float32), float32, [1, 56, 56, 64], [])}\n",
            "  buffer_map = {In_1: In, W1_1: W1, W2_1: W2, l2_conv_1: l2_conv} {\n",
            "  attr [IterVar(blockIdx.z: int32, (nullptr), \"ThreadIndex\", \"blockIdx.z\")] \"thread_extent\" = 56;\n",
            "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 1;\n",
            "  for (xx.inner: int32, 0, 56) {\n",
            "    attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 1;\n",
            "    for (ff.inner: int32, 0, 64) {\n",
            "      l2_conv_3: Buffer(l2_conv_2, float32, [200704], [])[(((blockIdx.z*3584) + (xx.inner*64)) + ff.inner)] = 0f32\n",
            "      for (l2_ry: int32, 0, 3) {\n",
            "        for (l2_rx: int32, 0, 3) {\n",
            "          for (l2_rc: int32, 0, 64) {\n",
            "            l2_conv_3[(((blockIdx.z*3584) + (xx.inner*64)) + ff.inner)] = (l2_conv_3[(((blockIdx.z*3584) + (xx.inner*64)) + ff.inner)] + (reduce(meta[tir.CommReducer][0], [(In_3: Buffer(In_2, float32, [154587], [])[(((((((blockIdx.z*2724) + (l2_ry*1362)) + (l1_ry: int32*681)) + (xx.inner*12)) + (l2_rx*6)) + (l1_rx: int32*3)) + l1_rc: int32)]*W1_3: Buffer(W1_2, float32, [1728], [])[((((l2_rc*27) + (l1_ry*9)) + (l1_rx*3)) + l1_rc)])], [IterVar(l1_ry, [0:3], \"CommReduce\", \"\"), IterVar(l1_rx, [0:3], \"CommReduce\", \"\"), IterVar(l1_rc, [0:3], \"CommReduce\", \"\")], 0, [])*W2_3: Buffer(W2_2, float32, [36864], [])[((((ff.inner*576) + (l2_ry*192)) + (l2_rx*64)) + l2_rc)]))\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# tile consts\n",
        "tile = 8\n",
        "num_thread = 8\n",
        "block_factor = tile * num_thread\n",
        "step = 8\n",
        "vthread = 2\n",
        "\n",
        "s = te.create_schedule(conv2.op)\n",
        "# SI = s.cache_read(In, \"shared\", [conv1])\n",
        "# SW1 = s.cache_read(W1, \"shared\", [conv1])\n",
        "# LI = s.cache_read(SI, \"local\", [conv1])\n",
        "# LW1 = s.cache_read(SW1, \"local\", [conv1])\n",
        "# LC1 = s.cache_write(conv1, \"shared\")\n",
        "\n",
        "# SA = s.cache_read(conv1, \"shared\", [conv2])\n",
        "# SW2 = s.cache_read(W2, \"shared\", [conv2])\n",
        "# LA = s.cache_read(SA, \"local\", [conv2])\n",
        "# LW2 = s.cache_read(SW2, \"local\", [conv2])\n",
        "# LC2 = s.cache_write(conv2, \"local\")\n",
        "\n",
        "s[conv1].compute_at(s[conv2], conv2.op.axis[2])\n",
        "s[conv1].compute_inline()  # compute Apad inline\n",
        "\n",
        "# Get the GPU thread indices\n",
        "block_x = te.thread_axis(\"blockIdx.x\")\n",
        "block_y = te.thread_axis(\"blockIdx.y\")\n",
        "block_z = te.thread_axis(\"blockIdx.z\")\n",
        "thread_x = te.thread_axis((0, num_thread), \"threadIdx.x\")\n",
        "thread_y = te.thread_axis((0, num_thread), \"threadIdx.y\")\n",
        "thread_xz = te.thread_axis((0, vthread), \"vthread\", name=\"vx\")\n",
        "thread_yz = te.thread_axis((0, vthread), \"vthread\", name=\"vy\")\n",
        "\n",
        "# Split the workloads\n",
        "hi, wi, fi, ni = s[conv2].op.axis\n",
        "bz = s[conv2].fuse(hi, wi)\n",
        "by, fi = s[conv2].split(fi, factor=block_factor)\n",
        "bx, ni = s[conv2].split(ni, factor=block_factor)\n",
        "\n",
        "# Bind the iteration variables to GPU thread indices\n",
        "s[conv2].bind(bz, block_z)\n",
        "s[conv2].bind(by, block_y)\n",
        "s[conv2].bind(bx, block_x)\n",
        "\n",
        "\n",
        "\n",
        "print(tvm.lower(s, [In, W1, W2, conv2], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@main = primfn(In_1: handle, W1_1: handle, W2_1: handle, l2_conv_1: handle) -> ()\n",
            "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
            "  buffers = {In: Buffer(In_2: Pointer(float32), float32, [227, 227, 3, 1], []),\n",
            "             W1: Buffer(W1_2: Pointer(float32), float32, [3, 3, 3, 64], []),\n",
            "             W2: Buffer(W2_2: Pointer(float32), float32, [3, 3, 64, 64], []),\n",
            "             l2_conv: Buffer(l2_conv_2: Pointer(float32), float32, [56, 56, 64, 1], [])}\n",
            "  buffer_map = {In_1: In, W1_1: W1, W2_1: W2, l2_conv_1: l2_conv} {\n",
            "  allocate(In.shared: Pointer(shared float32), float32, [2304]), storage_scope = shared;\n",
            "  allocate(In.shared.local: Pointer(local float32), float32, [588]), storage_scope = local;\n",
            "  allocate(W1.shared.local: Pointer(local float32), float32, [1728]), storage_scope = local;\n",
            "  allocate(l1_conv.d.shared.local: Pointer(local float32), float32, [2304]), storage_scope = local;\n",
            "  allocate(W2.shared.local: Pointer(local float32), float32, [2304]), storage_scope = local;\n",
            "  allocate(l2_conv.local: Pointer(local float32), float32, [16]), storage_scope = local {\n",
            "    for (ax0: int32, 0, 7) {\n",
            "      for (ax1: int32, 0, 7) {\n",
            "        for (ax2: int32, 0, 3) {\n",
            "          for (ax3: int32, 0, 4) {\n",
            "            if @tir.likely(((((vx: int32*32) + (threadIdx.x: int32*4)) + ax3) < 1), dtype=bool) {\n",
            "              In.shared_1: Buffer(In.shared, float32, [588], [], scope=\"shared\")[((((ax0*84) + (ax1*12)) + (ax2*4)) + ax3)] = In_3: Buffer(In_2, float32, [154587], [])[((((((((floordiv(blockIdx.z: int32, 56)*2724) + (ax0*681)) + (vx*32)) + (floormod(blockIdx.z, 56)*12)) + (threadIdx.x*4)) + (ax1*3)) + ax2) + ax3)]\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_1: int32, 0, 7) {\n",
            "      for (ax1_1: int32, 0, 7) {\n",
            "        for (ax2_1: int32, 0, 3) {\n",
            "          for (ax3_1: int32, 0, 4) {\n",
            "            if @tir.likely(((((vx*32) + (threadIdx.x*4)) + ax3_1) < 1), dtype=bool) {\n",
            "              let cse_var_1: int32 = ((((ax0_1*84) + (ax1_1*12)) + (ax2_1*4)) + ax3_1)\n",
            "              In.shared.local_1: Buffer(In.shared.local, float32, [588], [], scope=\"local\")[cse_var_1] = In.shared_1[cse_var_1]\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_2: int32, 0, 3) {\n",
            "      for (ax1_2: int32, 0, 3) {\n",
            "        for (ax2_2: int32, 0, 3) {\n",
            "          for (ax3_2: int32, 0, 64) {\n",
            "            let cse_var_2: int32 = ((((ax0_2*576) + (ax1_2*192)) + (ax2_2*64)) + ax3_2)\n",
            "            In.shared_2: Buffer(In.shared, float32, [1728], [], scope=\"shared\")[cse_var_2] = W1_3: Buffer(W1_2, float32, [1728], [])[cse_var_2]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_3: int32, 0, 3) {\n",
            "      for (ax1_3: int32, 0, 3) {\n",
            "        for (ax2_3: int32, 0, 3) {\n",
            "          for (ax3_3: int32, 0, 64) {\n",
            "            let cse_var_3: int32 = ((((ax0_3*576) + (ax1_3*192)) + (ax2_3*64)) + ax3_3)\n",
            "            W1.shared.local_1: Buffer(W1.shared.local, float32, [1728], [], scope=\"local\")[cse_var_3] = In.shared_2[cse_var_3]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (yy.c: int32, 0, 3) {\n",
            "      for (xx.c: int32, 0, 3) {\n",
            "        for (ff.c: int32, 0, 64) {\n",
            "          for (nn.c: int32, 0, 4) {\n",
            "            In.shared_3: Buffer(In.shared, float32, [2304], [], scope=\"shared\")[((((yy.c*768) + (xx.c*256)) + (ff.c*4)) + nn.c)] = 0f32\n",
            "            if @tir.likely(((((vx*32) + (threadIdx.x*4)) + nn.c) < 1), dtype=bool) {\n",
            "              for (l1_ry: int32, 0, 3) {\n",
            "                for (l1_rx: int32, 0, 3) {\n",
            "                  for (l1_rc: int32, 0, 3) {\n",
            "                    let cse_var_4: int32 = ((((yy.c*768) + (xx.c*256)) + (ff.c*4)) + nn.c)\n",
            "                    In.shared_3[cse_var_4] = (In.shared_3[cse_var_4] + (In.shared.local_1[((((((yy.c*168) + (l1_ry*84)) + (xx.c*24)) + (l1_rx*12)) + (l1_rc*4)) + nn.c)]*W1.shared.local_1[((((l1_ry*576) + (l1_rx*192)) + (l1_rc*64)) + ff.c)]))\n",
            "                  }\n",
            "                }\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_4: int32, 0, 3) {\n",
            "      for (ax1_4: int32, 0, 3) {\n",
            "        for (ax2_4: int32, 0, 64) {\n",
            "          for (ax3_4: int32, 0, 4) {\n",
            "            if @tir.likely(((((vx*32) + (threadIdx.x*4)) + ax3_4) < 1), dtype=bool) {\n",
            "              let cse_var_5: int32 = ((((ax0_4*768) + (ax1_4*256)) + (ax2_4*4)) + ax3_4)\n",
            "              l1_conv.d.shared.local_1: Buffer(l1_conv.d.shared.local, float32, [2304], [], scope=\"local\")[cse_var_5] = In.shared_4: Buffer(In.shared, float32, [2304], [], scope=\"shared\")[cse_var_5]\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_5: int32, 0, 3) {\n",
            "      for (ax1_5: int32, 0, 3) {\n",
            "        for (ax2_5: int32, 0, 64) {\n",
            "          for (ax3_5: int32, 0, 4) {\n",
            "            In.shared_5: Buffer(In.shared, float32, [2304], [], scope=\"shared\")[((((ax0_5*768) + (ax1_5*256)) + (ax2_5*4)) + ax3_5)] = W2_3: Buffer(W2_2, float32, [36864], [])[((((((ax0_5*12288) + (ax1_5*4096)) + (ax2_5*64)) + (vy: int32*32)) + (threadIdx.y: int32*4)) + ax3_5)]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ax0_6: int32, 0, 3) {\n",
            "      for (ax1_6: int32, 0, 3) {\n",
            "        for (ax2_6: int32, 0, 64) {\n",
            "          for (ax3_6: int32, 0, 4) {\n",
            "            let cse_var_6: int32 = ((((ax0_6*768) + (ax1_6*256)) + (ax2_6*4)) + ax3_6)\n",
            "            W2.shared.local_1: Buffer(W2.shared.local, float32, [2304], [], scope=\"local\")[cse_var_6] = In.shared_5[cse_var_6]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    for (ff.c_1: int32, 0, 4) {\n",
            "      for (nn.c_1: int32, 0, 4) {\n",
            "        l2_conv.local_1: Buffer(l2_conv.local, float32, [16], [], scope=\"local\")[((ff.c_1*4) + nn.c_1)] = 0f32\n",
            "        if @tir.likely(((((vx*32) + (threadIdx.x*4)) + nn.c_1) < 1), dtype=bool) {\n",
            "          for (l2_ry: int32, 0, 3) {\n",
            "            for (l2_rx: int32, 0, 3) {\n",
            "              for (l2_rc: int32, 0, 64) {\n",
            "                let cse_var_8: int32 = ((ff.c_1*4) + nn.c_1)\n",
            "                let cse_var_7: int32 = (((l2_ry*768) + (l2_rx*256)) + (l2_rc*4))\n",
            "                l2_conv.local_1[cse_var_8] = (l2_conv.local_1[cse_var_8] + (l1_conv.d.shared.local_1[(cse_var_7 + nn.c_1)]*W2.shared.local_1[(cse_var_7 + ff.c_1)]))\n",
            "              }\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    attr [IterVar(blockIdx.z, (nullptr), \"ThreadIndex\", \"blockIdx.z\")] \"thread_extent\" = 3136;\n",
            "    attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 1;\n",
            "    attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 1;\n",
            "    attr [IterVar(threadIdx.y, [0:8], \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 8;\n",
            "    attr [IterVar(threadIdx.x, [0:8], \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 8;\n",
            "    for (ff.inner.inner.inner: int32, 0, 4) {\n",
            "      for (nn.inner.inner.inner: int32, 0, 4) {\n",
            "        if @tir.likely((((threadIdx.x*4) + nn.inner.inner.inner) < 1), dtype=bool) {\n",
            "          let cse_var_9: int32 = ((ff.inner.inner.inner*4) + nn.inner.inner.inner)\n",
            "           {\n",
            "            l2_conv_3: Buffer(l2_conv_2, float32, [200704], [])[(((((blockIdx.z*64) + (threadIdx.y*4)) + (threadIdx.x*4)) + ff.inner.inner.inner) + nn.inner.inner.inner)] = l2_conv.local_1[cse_var_9]\n",
            "            l2_conv_3[((((((blockIdx.z*64) + (threadIdx.y*4)) + (threadIdx.x*4)) + ff.inner.inner.inner) + nn.inner.inner.inner) + 32)] = l2_conv.local_1[cse_var_9]\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# tile consts\n",
        "tile = 8\n",
        "num_thread = 8\n",
        "block_factor = tile * num_thread\n",
        "step = 8\n",
        "vthread = 2\n",
        "\n",
        "s = te.create_schedule(conv2.op)\n",
        "SI = s.cache_read(In, \"shared\", [conv1])\n",
        "SW1 = s.cache_read(W1, \"shared\", [conv1])\n",
        "LI = s.cache_read(SI, \"local\", [conv1])\n",
        "LW1 = s.cache_read(SW1, \"local\", [conv1])\n",
        "LC1 = s.cache_write(conv1, \"shared\")\n",
        "\n",
        "SA = s.cache_read(conv1, \"shared\", [conv2])\n",
        "SW2 = s.cache_read(W2, \"shared\", [conv2])\n",
        "LA = s.cache_read(SA, \"local\", [conv2])\n",
        "LW2 = s.cache_read(SW2, \"local\", [conv2])\n",
        "LC2 = s.cache_write(conv2, \"local\")\n",
        "\n",
        "s[conv1].compute_at(s[conv2], conv2.op.axis[2])\n",
        "s[conv1].compute_inline()  # compute Apad inline\n",
        "\n",
        "# Get the GPU thread indices\n",
        "block_x = te.thread_axis(\"blockIdx.x\")\n",
        "block_y = te.thread_axis(\"blockIdx.y\")\n",
        "block_z = te.thread_axis(\"blockIdx.z\")\n",
        "thread_x = te.thread_axis((0, num_thread), \"threadIdx.x\")\n",
        "thread_y = te.thread_axis((0, num_thread), \"threadIdx.y\")\n",
        "thread_xz = te.thread_axis((0, vthread), \"vthread\", name=\"vx\")\n",
        "thread_yz = te.thread_axis((0, vthread), \"vthread\", name=\"vy\")\n",
        "\n",
        "# Split the workloads\n",
        "hi, wi, fi, ni = s[conv2].op.axis\n",
        "bz = s[conv2].fuse(hi, wi)\n",
        "by, fi = s[conv2].split(fi, factor=block_factor)\n",
        "bx, ni = s[conv2].split(ni, factor=block_factor)\n",
        "\n",
        "# Bind the iteration variables to GPU thread indices\n",
        "s[conv2].bind(bz, block_z)\n",
        "s[conv2].bind(by, block_y)\n",
        "s[conv2].bind(bx, block_x)\n",
        "\n",
        "\n",
        "\n",
        "tyz, fi = s[conv2].split(fi, nparts=vthread)  # virtual thread split\n",
        "txz, ni = s[conv2].split(ni, nparts=vthread)  # virtual thread split\n",
        "ty, fi = s[conv2].split(fi, nparts=num_thread)\n",
        "tx, ni = s[conv2].split(ni, nparts=num_thread)\n",
        "s[conv2].reorder(bz, by, bx, tyz, txz, ty, tx, fi, ni)\n",
        "\n",
        "s[conv2].bind(tyz, thread_yz)\n",
        "s[conv2].bind(txz, thread_xz)\n",
        "s[conv2].bind(ty, thread_y)\n",
        "s[conv2].bind(tx, thread_x)\n",
        "\n",
        "print(tvm.lower(s, [In, W1, W2, conv2], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  6: TVMFuncCall\n  5: _ZN3tvm7runtime13PackedFuncObj\n  4: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  3: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)\n  2: tvm::ScheduleToModule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply)\n  1: tvm::te::InferBound(tvm::te::Schedule const&)\n  0: tvm::te::PassDownDomain(tvm::te::Stage const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*, tvm::arith::Analyzer*, bool)\n  File \"/home/piai/tool/tvm/src/te/schedule/message_passing.cc\", line 166\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (allow_missing) is false: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [26], line 100\u001b[0m\n\u001b[1;32m     72\u001b[0m s[LC2]\u001b[39m.\u001b[39mreorder(rco, ry, rx, rci, fi, ni)\n\u001b[1;32m     74\u001b[0m \u001b[39m# # Attach computation to iteration variables\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# s[AA].compute_at(s[BL], rx)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# s[WW].compute_at(s[BL], rx)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m# s[WW].bind(tx, thread_x)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# s[WW].vectorize(fi)  # vectorize memory load\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[39mprint\u001b[39m(tvm\u001b[39m.\u001b[39;49mlower(s, [In, W1, W2, conv2], simple_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n",
            "File \u001b[0;32m~/tool/tvm/python/tvm/driver/build_module.py:134\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(inp, args, name, binds, simple_mode)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m ffi\u001b[39m.\u001b[39mlower_primfunc(inp, name, simple_mode)\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inp, te\u001b[39m.\u001b[39mSchedule):\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m ffi\u001b[39m.\u001b[39;49mlower_schedule(inp, args, name, binds, simple_mode)\n\u001b[1;32m    135\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected input to be an IRModule, PrimFunc or te.Schedule, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(inp)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m )\n",
            "File \u001b[0;32m~/tool/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m args\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  6: TVMFuncCall\n  5: _ZN3tvm7runtime13PackedFuncObj\n  4: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)>::AssignTypedLambda<tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}>(tvm::{lambda(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, tvm::runtime::String const&, tvm::runtime::Map<tvm::te::Tensor, tvm::tir::Buffer, void, void> const&, bool)#5}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*) const\n  3: tvm::LowerSchedule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply, bool)\n  2: tvm::ScheduleToModule(tvm::te::Schedule, tvm::runtime::Array<tvm::runtime::ObjectRef, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_map<tvm::te::Tensor, tvm::tir::Buffer, std::hash<tvm::te::Tensor>, std::equal_to<tvm::te::Tensor>, std::allocator<std::pair<tvm::te::Tensor const, tvm::tir::Buffer> > > const&, tvm::GlobalVarSupply)\n  1: tvm::te::InferBound(tvm::te::Schedule const&)\n  0: tvm::te::PassDownDomain(tvm::te::Stage const&, std::unordered_map<tvm::tir::IterVar, tvm::Range, std::hash<tvm::tir::IterVar>, std::equal_to<tvm::tir::IterVar>, std::allocator<std::pair<tvm::tir::IterVar const, tvm::Range> > >*, tvm::arith::Analyzer*, bool)\n  File \"/home/piai/tool/tvm/src/te/schedule/message_passing.cc\", line 166\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (allow_missing) is false: "
          ]
        }
      ],
      "source": [
        "# tile consts\n",
        "tile = 8\n",
        "num_thread = 8\n",
        "block_factor = tile * num_thread\n",
        "step = 8\n",
        "vthread = 2\n",
        "\n",
        "s = te.create_schedule(conv2.op)\n",
        "SI = s.cache_read(In, \"shared\", [conv1])\n",
        "SW1 = s.cache_read(W1, \"shared\", [conv1])\n",
        "LI = s.cache_read(SI, \"local\", [conv1])\n",
        "LW1 = s.cache_read(SW1, \"local\", [conv1])\n",
        "LC1 = s.cache_write(conv1, \"shared\")\n",
        "\n",
        "SA = s.cache_read(conv1, \"shared\", [conv2])\n",
        "SW2 = s.cache_read(W2, \"shared\", [conv2])\n",
        "LA = s.cache_read(SA, \"local\", [conv2])\n",
        "LW2 = s.cache_read(SW2, \"local\", [conv2])\n",
        "LC2 = s.cache_write(conv2, \"local\")\n",
        "\n",
        "s[conv1].compute_at(s[conv2], conv2.op.axis[2])\n",
        "s[conv1].compute_inline()  # compute Apad inline\n",
        "\n",
        "# Get the GPU thread indices\n",
        "block_x = te.thread_axis(\"blockIdx.x\")\n",
        "block_y = te.thread_axis(\"blockIdx.y\")\n",
        "block_z = te.thread_axis(\"blockIdx.z\")\n",
        "thread_x = te.thread_axis((0, num_thread), \"threadIdx.x\")\n",
        "thread_y = te.thread_axis((0, num_thread), \"threadIdx.y\")\n",
        "thread_xz = te.thread_axis((0, vthread), \"vthread\", name=\"vx\")\n",
        "thread_yz = te.thread_axis((0, vthread), \"vthread\", name=\"vy\")\n",
        "\n",
        "# Split the workloads\n",
        "hi, wi, fi, ni = s[conv1].op.axis\n",
        "bz = s[conv1].fuse(hi, wi)\n",
        "by, fi = s[conv1].split(fi, factor=block_factor)\n",
        "bx, ni = s[conv1].split(ni, factor=block_factor)\n",
        "\n",
        "# Bind the iteration variables to GPU thread indices\n",
        "s[conv1].bind(bz, block_z)\n",
        "s[conv1].bind(by, block_y)\n",
        "s[conv1].bind(bx, block_x)\n",
        "\n",
        "# Split the workloads\n",
        "hi, wi, fi, ni = s[conv2].op.axis\n",
        "bz = s[conv2].fuse(hi, wi)\n",
        "by, fi = s[conv2].split(fi, factor=block_factor)\n",
        "bx, ni = s[conv2].split(ni, factor=block_factor)\n",
        "\n",
        "# Bind the iteration variables to GPU thread indices\n",
        "s[conv2].bind(bz, block_z)\n",
        "s[conv2].bind(by, block_y)\n",
        "s[conv2].bind(bx, block_x)\n",
        "\n",
        "\n",
        "tyz, fi = s[conv2].split(fi, nparts=vthread)  # virtual thread split\n",
        "txz, ni = s[conv2].split(ni, nparts=vthread)  # virtual thread split\n",
        "ty, fi = s[conv2].split(fi, nparts=num_thread)\n",
        "tx, ni = s[conv2].split(ni, nparts=num_thread)\n",
        "s[conv2].reorder(bz, by, bx, tyz, txz, ty, tx, fi, ni)\n",
        "\n",
        "s[conv2].bind(tyz, thread_yz)\n",
        "s[conv2].bind(txz, thread_xz)\n",
        "s[conv2].bind(ty, thread_y)\n",
        "s[conv2].bind(tx, thread_x)\n",
        "\n",
        "# Schedule BL local write\n",
        "s[LC2].compute_at(s[conv2], tx)\n",
        "yi, xi, fi, ni = s[LC2].op.axis\n",
        "ry, rx, rc = s[LC2].op.reduce_axis\n",
        "rco, rci = s[LC2].split(rc, factor=step)\n",
        "s[LC2].reorder(rco, ry, rx, rci, fi, ni)\n",
        "\n",
        "# # Attach computation to iteration variables\n",
        "# s[AA].compute_at(s[BL], rx)\n",
        "# s[WW].compute_at(s[BL], rx)\n",
        "# s[AL].compute_at(s[BL], rci)\n",
        "# s[WL].compute_at(s[BL], rci)\n",
        "\n",
        "# # Schedule for A's shared memory load\n",
        "yi, xi, ci, ni = s[conv1].op.axis\n",
        "ty, ci = s[conv1].split(ci, nparts=num_thread)\n",
        "tx, ni = s[conv1].split(ni, nparts=num_thread)\n",
        "_, ni = s[conv1].split(ni, factor=4)\n",
        "s[conv1].reorder(ty, tx, yi, xi, ci, ni)\n",
        "s[conv1].bind(ty, thread_y)\n",
        "s[conv1].bind(tx, thread_x)\n",
        "s[conv1].vectorize(ni)  # vectorize memory load\n",
        "\n",
        "# # Schedule for W's shared memory load\n",
        "# yi, xi, ci, fi = s[WW].op.axis\n",
        "# ty, ci = s[WW].split(ci, nparts=num_thread)\n",
        "# tx, fi = s[WW].split(fi, nparts=num_thread)\n",
        "# _, fi = s[WW].split(fi, factor=4)\n",
        "# s[WW].reorder(ty, tx, yi, xi, ci, fi)\n",
        "# s[WW].bind(ty, thread_y)\n",
        "# s[WW].bind(tx, thread_x)\n",
        "# s[WW].vectorize(fi)  # vectorize memory load\n",
        "\n",
        "print(tvm.lower(s, [In, W1, W2, conv2], simple_mode=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.15 ('jax')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "a80d245055573cb444f9632de3654ee2e5037a494ec6959515d8eaba75b4a6a8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
